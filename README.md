# âš½ Football Live Score & Stats Web Application

A comprehensive real-time football live score tracking application with Python backend for data crawling and SvelteKit frontend for beautiful, responsive UI.

## ğŸ“‹ Table of Contents

- [Features](#features)
- [Architecture](#architecture)
- [Tech Stack](#tech-stack)
- [Project Structure](#project-structure)
- [Quick Start](#quick-start)
- [Development](#development)
- [Deployment](#deployment)
- [API Documentation](#api-documentation)
- [Contributing](#contributing)
- [License](#license)

## âœ¨ Features

### Core Features
- ğŸ”´ **Real-time Live Scores** - WebSocket-powered live updates
- âš¡ **Fast Performance** - Optimized with Redis caching
- ğŸ“Š **Match Statistics** - Detailed stats, events, lineups
- ğŸ† **League Tables** - Current standings and rankings
- ğŸ“… **Fixtures** - Upcoming and past matches
- ğŸ‘¤ **User Accounts** - Authentication with JWT
- â­ **Favorites** - Save your favorite teams/leagues
- ğŸ“± **Responsive Design** - Works on all devices

### Technical Features
- **Distributed Crawling** - Celery-based background tasks
- **Hybrid Database** - PostgreSQL + MongoDB + Redis
- **Anti-Detection** - Proxy rotation, user-agent rotation
- **Auto-Retry** - Exponential backoff for failed requests
- **Monitoring** - Prometheus metrics, structured logging
- **Scalable** - Microservices architecture

## ğŸ—ï¸ Architecture

```
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚                     FRONTEND LAYER                      â”‚
â”‚  SvelteKit App (SSR/CSR) + WebSocket Client            â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
                         â”‚ HTTP/REST + WebSocket
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â–¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚                   API GATEWAY (Nginx)                   â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
                         â”‚
        â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”´â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
        â”‚                                 â”‚
â”Œâ”€â”€â”€â”€â”€â”€â”€â–¼â”€â”€â”€â”€â”€â”€â”€â”€â”              â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â–¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚  FastAPI App   â”‚              â”‚  WebSocket Serverâ”‚
â”‚  (REST API)    â”‚              â”‚  (Real-time)     â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”˜              â””â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
        â”‚                                â”‚
        â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
                     â”‚
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â–¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚              DATABASE & CACHE LAYER                  â”‚
â”‚  â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”  â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”  â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”        â”‚
â”‚  â”‚ PostgreSQL â”‚  â”‚ MongoDB  â”‚  â”‚  Redis   â”‚        â”‚
â”‚  â”‚ (metadata) â”‚  â”‚ (matches)â”‚  â”‚ (cache)  â”‚        â”‚
â”‚  â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜  â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜  â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜        â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
                     â”‚
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â–¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚                 CRAWLING LAYER                       â”‚
â”‚  Celery Workers + Scrapy + Playwright               â”‚
â”‚  â”œâ”€ Live Score Crawler (every 30s)                  â”‚
â”‚  â”œâ”€ Match Events Crawler (every 10s)                â”‚
â”‚  â”œâ”€ Fixtures Crawler (daily)                        â”‚
â”‚  â””â”€ Stats Crawler (hourly)                          â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
```

## ğŸ› ï¸ Tech Stack

### Backend
- **Framework**: FastAPI (async Python web framework)
- **Task Queue**: Celery + Redis
- **Web Scraping**: Scrapy, Playwright, BeautifulSoup, httpx
- **Databases**:
  - PostgreSQL (users, teams, leagues)
  - MongoDB (matches, events, stats)
  - Redis (cache, pub/sub, broker)
- **Authentication**: JWT (python-jose, passlib)
- **ORM**: SQLAlchemy (async), Motor (async MongoDB)

### Frontend
- **Framework**: SvelteKit (SSR + CSR)
- **Styling**: TailwindCSS
- **HTTP Client**: Axios
- **Real-time**: Native WebSocket
- **Build Tool**: Vite

### Infrastructure
- **Containerization**: Docker + Docker Compose
- **Reverse Proxy**: Nginx
- **Monitoring**: Prometheus + Grafana (optional)
- **Logging**: Structlog (structured JSON logs)

## ğŸ“ Project Structure

```
football-live-and-stats/
â”œâ”€â”€ backend/                    # Python backend
â”‚   â”œâ”€â”€ api/                    # FastAPI application
â”‚   â”‚   â”œâ”€â”€ core/              # Config, database, security
â”‚   â”‚   â”œâ”€â”€ models/            # SQLAlchemy models
â”‚   â”‚   â”œâ”€â”€ routes/            # API endpoints
â”‚   â”‚   â”œâ”€â”€ schemas/           # Pydantic schemas
â”‚   â”‚   â”œâ”€â”€ services/          # Business logic
â”‚   â”‚   â””â”€â”€ main.py            # FastAPI app entry
â”‚   â”œâ”€â”€ crawlers/              # Web crawlers
â”‚   â”‚   â”œâ”€â”€ base.py            # Base crawler class
â”‚   â”‚   â”œâ”€â”€ flashscore.py      # FlashScore crawler
â”‚   â”‚   â””â”€â”€ parsers/           # HTML parsers
â”‚   â”œâ”€â”€ tasks/                 # Celery tasks
â”‚   â”‚   â”œâ”€â”€ celery_app.py      # Celery config
â”‚   â”‚   â”œâ”€â”€ live_scores.py     # Live score tasks
â”‚   â”‚   â”œâ”€â”€ fixtures.py        # Fixture tasks
â”‚   â”‚   â””â”€â”€ stats.py           # Stats tasks
â”‚   â”œâ”€â”€ requirements.txt       # Python dependencies
â”‚   â”œâ”€â”€ Dockerfile             # Backend Docker image
â”‚   â””â”€â”€ docker-compose.yml     # Full stack setup
â”‚
â”œâ”€â”€ frontend/                   # SvelteKit frontend
â”‚   â”œâ”€â”€ src/
â”‚   â”‚   â”œâ”€â”€ lib/
â”‚   â”‚   â”‚   â”œâ”€â”€ components/    # Svelte components
â”‚   â”‚   â”‚   â”œâ”€â”€ stores/        # State management
â”‚   â”‚   â”‚   â””â”€â”€ types/         # TypeScript types
â”‚   â”‚   â”œâ”€â”€ routes/            # SvelteKit pages
â”‚   â”‚   â”œâ”€â”€ app.css            # Global styles
â”‚   â”‚   â””â”€â”€ app.html           # HTML template
â”‚   â”œâ”€â”€ package.json           # Node dependencies
â”‚   â”œâ”€â”€ svelte.config.js       # SvelteKit config
â”‚   â”œâ”€â”€ tailwind.config.js     # Tailwind config
â”‚   â””â”€â”€ vite.config.ts         # Vite config
â”‚
â””â”€â”€ README.md                   # This file
```

## ğŸš€ Quick Start

### Prerequisites

- Docker & Docker Compose (recommended)
- OR:
  - Python 3.11+
  - Node.js 18+
  - PostgreSQL 15+
  - MongoDB 7+
  - Redis 7+

### Option 1: Docker Compose (Recommended)

1. **Clone the repository**
```bash
git clone <repository-url>
cd football-live-and-stats
```

2. **Create environment files**
```bash
# Backend
cd backend
cp .env.example .env
# Edit .env with your configuration

# Frontend
cd ../frontend
cp .env.example .env
# Edit .env (use default values for Docker)
```

3. **Start all services**
```bash
cd backend
docker-compose up -d
```

4. **Access the application**
- Frontend: http://localhost:5173
- Backend API: http://localhost:8000
- API Docs: http://localhost:8000/api/v1/docs

### Option 2: Manual Setup

#### Backend

```bash
cd backend

# Create virtual environment
python -m venv venv
source venv/bin/activate  # On Windows: venv\Scripts\activate

# Install dependencies
pip install -r requirements.txt
playwright install chromium

# Setup environment
cp .env.example .env
# Edit .env with your database URLs

# Start services
uvicorn api.main:app --reload  # Terminal 1
celery -A tasks.celery_app worker --loglevel=info  # Terminal 2
celery -A tasks.celery_app beat --loglevel=info  # Terminal 3
```

#### Frontend

```bash
cd frontend

# Install dependencies
npm install

# Setup environment
cp .env.example .env
# Edit .env with backend URL

# Start dev server
npm run dev
```

## ğŸ’» Development

### Backend Development

```bash
cd backend

# Run tests
pytest

# Format code
black .

# Type checking
mypy .

# Linting
flake8 .

# Database migrations
alembic revision --autogenerate -m "description"
alembic upgrade head
```

### Frontend Development

```bash
cd frontend

# Run dev server
npm run dev

# Build for production
npm run build

# Preview production build
npm run preview

# Type checking
npm run check

# Linting
npm run lint

# Format code
npm run format
```

### Adding New Crawlers

1. Create new crawler in `backend/crawlers/`
2. Extend `BaseCrawler` class
3. Implement required methods: `crawl_match()`, `crawl_fixtures()`, etc.
4. Add Celery task in `backend/tasks/`
5. Update Celery beat schedule

Example:
```python
# backend/crawlers/new_source.py
from crawlers.base import BaseCrawler

class NewSourceCrawler(BaseCrawler):
    async def crawl_match(self, match_id: str):
        # Implementation
        pass
```

## ğŸ“Š API Documentation

API documentation is automatically generated by FastAPI:

- **Swagger UI**: http://localhost:8000/api/v1/docs
- **ReDoc**: http://localhost:8000/api/v1/redoc

### Key Endpoints

```
Authentication:
POST   /api/v1/auth/register
POST   /api/v1/auth/login
GET    /api/v1/auth/me

Matches:
GET    /api/v1/matches/live
GET    /api/v1/matches/today
GET    /api/v1/matches/{id}

Leagues:
GET    /api/v1/leagues
GET    /api/v1/leagues/{id}/table

WebSocket:
WS     /ws
```

## ğŸŒ Deployment

### Production Checklist

- [ ] Set `DEBUG=False` in backend `.env`
- [ ] Use strong `SECRET_KEY`
- [ ] Configure production databases (not Docker)
- [ ] Enable HTTPS (SSL/TLS)
- [ ] Set up reverse proxy (Nginx)
- [ ] Configure CORS for production domain
- [ ] Set up monitoring (Prometheus + Grafana)
- [ ] Configure log aggregation
- [ ] Set up automated backups
- [ ] Configure rate limiting
- [ ] Use CDN for static assets

### Deployment Options

#### AWS

- **Compute**: ECS Fargate / EC2
- **Database**: RDS PostgreSQL, DocumentDB/MongoDB Atlas
- **Cache**: ElastiCache Redis
- **Load Balancer**: ALB
- **Static Assets**: S3 + CloudFront

#### Vercel (Frontend)

```bash
cd frontend
npm install -g vercel
vercel
```

#### Heroku (Backend)

```bash
cd backend
heroku create
heroku addons:create heroku-postgresql
heroku addons:create heroku-redis
git push heroku main
```

## ğŸ“ˆ Performance

### Benchmarks (Expected)

- API Response Time: < 100ms (p95)
- WebSocket Latency: < 50ms
- Live Score Update Delay: 10-30s
- Page Load Time: < 2s (FCP)
- Concurrent Users: 10,000+

### Optimization Tips

1. **Caching**: Use Redis for frequently accessed data
2. **Database Indexing**: Optimize MongoDB and PostgreSQL queries
3. **CDN**: Serve static assets via CDN
4. **Compression**: Enable gzip/brotli compression
5. **Connection Pooling**: Configure database connection pools
6. **Lazy Loading**: Implement infinite scroll for large lists

## ğŸ”’ Security

- JWT authentication with secure token storage
- Password hashing with bcrypt
- SQL injection prevention (parameterized queries)
- XSS protection (input sanitization)
- CORS configuration
- Rate limiting on API endpoints
- Environment variables for secrets
- HTTPS in production

## ğŸ› Troubleshooting

### Common Issues

**Issue**: Database connection failed
- Check if databases are running: `docker-compose ps`
- Verify connection strings in `.env`

**Issue**: Celery worker not processing tasks
- Check Redis connection
- Verify Celery broker URL
- Check worker logs: `docker-compose logs celery_worker`

**Issue**: WebSocket not connecting
- Verify WebSocket URL in frontend `.env`
- Check CORS configuration
- Inspect browser console for errors

**Issue**: Crawling fails with 403 error
- Enable proxy rotation
- Check user-agent rotation
- Verify source website is accessible

## ğŸ“ License

MIT License - See LICENSE file for details

## ğŸ¤ Contributing

Contributions are welcome! Please follow these steps:

1. Fork the repository
2. Create feature branch (`git checkout -b feature/AmazingFeature`)
3. Commit changes (`git commit -m 'Add AmazingFeature'`)
4. Push to branch (`git push origin feature/AmazingFeature`)
5. Open Pull Request

## ğŸ“§ Contact

For questions or support, please open an issue on GitHub.

## ğŸ™ Acknowledgments

- Data sources: FlashScore, LiveScore, SofaScore
- Icons: Heroicons
- Inspiration: Various football score websites

---

**âš ï¸ Disclaimer**: This project is for educational purposes. Be respectful of website terms of service when crawling data. Consider using official APIs when available.

**ğŸ¯ Roadmap**:
- [ ] Player statistics and profiles
- [ ] Match predictions using ML
- [ ] Mobile apps (React Native)
- [ ] Push notifications
- [ ] Social features (comments, predictions)
- [ ] Multi-language support
- [ ] Dark mode
- [ ] Live match commentary
- [ ] Video highlights integration
